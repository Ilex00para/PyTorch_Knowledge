{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8707, -0.5997,  0.1352],\n",
      "        [ 1.0290,  2.0745,  0.3960],\n",
      "        [ 0.7930, -0.3834, -0.4516],\n",
      "        [-1.1449, -0.2077, -1.2865],\n",
      "        [ 3.4425, -0.1724,  1.0075]], device='cuda:0')\n",
      "tensor([[-0.7889,  0.0801,  0.4141,  0.2791, -0.7523,  0.3144],\n",
      "        [-0.3024, -0.6704, -1.1177,  0.5613,  0.4372,  0.3789],\n",
      "        [-0.8703,  0.2231,  0.3824,  0.1303, -0.6341,  0.1489],\n",
      "        [-0.3441, -0.2168,  0.0742, -0.3694,  0.4775,  0.4835],\n",
      "        [-1.4288,  0.5811,  0.4914,  0.9280, -1.9260, -0.1999]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.3643,  0.1788,  0.2529],\n",
      "        [ 0.3712, -0.2645, -0.3904],\n",
      "        [ 0.1849, -0.5663, -0.1792],\n",
      "        [ 0.1524,  0.0712,  0.2597],\n",
      "        [-0.5505,  0.4729,  0.0458],\n",
      "        [-0.3128,  0.0107,  0.3274]], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "sample = torch.randn(5, 3).to(device)\n",
    "\n",
    "print(sample)\n",
    "\n",
    "lins = nn.Linear(3, 6).to(device)\n",
    "\n",
    "print(lins(sample))   \n",
    "\n",
    "print(lins.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix 1: torch.Size([5, 3]) x Matrix 2: torch.Size([3, 5]) = torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "multi_ma = torch.mm(sample, sample.T)\n",
    "\n",
    "print(f'Matrix 1: {sample.shape} x Matrix 2: {sample.T.shape} = {multi_ma.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix 1: torch.Size([5, 3]) x Matrix 2: torch.Size([3, 5]) = torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "multi_ma = sample.mm( sample.T)\n",
    "\n",
    "print(f'Matrix 1: {sample.shape} x Matrix 2: {sample.T.shape} = {multi_ma.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 8], device='cuda:0', dtype=torch.int8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacob\\AppData\\Local\\Temp\\ipykernel_20548\\4077543698.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.tokens[torch.tensor(f).clone().detach()] = i\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "tensor([0, 1, 0], device='cuda:0', dtype=torch.int8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m ed_flowers \u001b[38;5;241m=\u001b[39m EncoderDecoder(token_tensor\u001b[38;5;241m=\u001b[39mtoken_tensor)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(ed_flowers\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43med_flowers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[55], line 22\u001b[0m, in \u001b[0;36mEncoderDecoder.encode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x:torch\u001b[38;5;241m.\u001b[39mtensor):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: tensor([0, 1, 0], device='cuda:0', dtype=torch.int8)"
     ]
    }
   ],
   "source": [
    "\n",
    "token_tensor = torch.zeros(size=(10**3, 3), dtype=torch.int8).to(device)\n",
    "\n",
    "counter = 0\n",
    "for male in range(0,10):\n",
    "    for female in range(0,10):\n",
    "        for aborted in range(0,10):\n",
    "            token_tensor[counter,:] = torch.tensor([male,female,aborted], requires_grad=False)\n",
    "            counter += 1\n",
    "\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, token_tensor) -> None:\n",
    "        super().__init__()\n",
    "        self.tokens = {}\n",
    "        for i, f in enumerate(token_tensor):\n",
    "            self.tokens[torch.tensor(f).clone().detach()] = i\n",
    "        \n",
    "        self.f = {value: key for key, value in self.tokens.items()}\n",
    "    \n",
    "    def encode(self, x:torch.tensor):\n",
    "        return self.tokens[x]\n",
    "\n",
    "    def decode(self, x:torch.long):\n",
    "        return self.f[x]\n",
    "\n",
    "\n",
    "ed_flowers = EncoderDecoder(token_tensor=token_tensor)\n",
    "\n",
    "print(ed_flowers.decode(8))\n",
    "print(ed_flowers.encode(torch.tensor([0,1,0],device='cuda', dtype=torch.int8)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-gpt",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
